---
title: "4_CDS_prediction_new_model"
author: "Min-Yao"
date: "8/23/2021"
output: html_document
---

```{r}
library(tidyverse)
library(keras)
library(tensorflow)
#tf$config$experimental$list_physical_devices()
```

```{r}
chr1 <- read_csv("AtSeqs/chr1_1_hot_plus_strand_all.csv.gz") %>%
  select(-pos) %>%
  data.matrix()

head(chr1)
```

```{r}
generator <- function(data, window, min_index, max_index,
                      shuffle = FALSE, batch_size = 32, step = 1) { #note window is really half window
  if (is.null(max_index))
    max_index <- nrow(data) - window - 1
  i <- min_index + window
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+window):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + window
      rows <- c(i:min(i+batch_size, max_index))
      i <<- i + length(rows)
    }
    
    samples <- array(0, dim = c(length(rows), 
                                (2 * window / step) + 1, # 2 X window becuase we are taking from either side of target
                                (dim(data)[[-1]]-1))) # don't include CDS in samples
    targets <- array(0, dim = c(length(rows)))
                     
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - window, rows[[j]] + window,  
                     length.out = dim(samples)[[2]])

#now each row of samples has data from before and after the focal base, getting progressively further away
      samples[j,,] <- data[indices, -1 ] # -1 = don't include CDS in samples
      
      targets[[j]] <- data[rows[[j]],1]
    }            
    
    list(samples, targets)
  }
}
```

```{r}
nrow(chr1) # 30427671
window <- 150
step <- 1
batch_size <- 512

train_gen <- generator(
  chr1,
  window = window,
  min_index = 1,
  max_index = 5000000,
  shuffle = TRUE,
  step = step, 
  batch_size = batch_size
)

val_gen = generator(
  chr1,
  window = window,
  min_index = 5000001,
  max_index = 5100000,
  step = step,
  batch_size = batch_size
)

test_gen <- generator(
  chr1,
  window = window,
  min_index = 6000001,
  max_index = 7000000,
  step = step,
  batch_size = batch_size
)

# This is how many steps to draw from `val_gen`
# in order to see the whole validation set:
val_steps <- (5100000 - 5000001 - window) / batch_size

  # This is how many steps to draw from `test_gen`
# in order to see the whole test set:
test_steps <- (7000000 - 6000001 - window) / batch_size
```
## Bi-directional GRU RNN

```{r}
model <- keras_model_sequential() %>% 
  bidirectional(layer_gru(units = 8, 
            input_shape = list(NULL, dim(chr1)[-1]-1),
            recurrent_activation = "sigmoid",
            use_bias = TRUE,
            reset_after = TRUE)) %>% 
  layer_dense(units = 1, activation="sigmoid")

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = "acc"
)

system.time(history <- model %>% fit(
  train_gen,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = val_gen,
  validation_steps = val_steps
))

model %>% evaluate(test_gen, steps=test_steps)

plot(history)
```

## 2 layer bidirectional GRU RNN

```{r}
model <- keras_model_sequential() %>% 
  bidirectional(layer_gru(units = 8, 
            input_shape = list(NULL, dim(chr1)[-1]-1),
            recurrent_activation = "sigmoid",
            use_bias = TRUE,
            reset_after = TRUE,
            return_sequences = TRUE)) %>% 
  bidirectional(layer_gru(units = 8, 
            input_shape = list(NULL, dim(chr1)[-1]-1),
            recurrent_activation = "sigmoid",
            use_bias = TRUE,
            reset_after = TRUE)) %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = "acc"
)

system.time(history <- model %>% fit(
  train_gen,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = val_gen,
  validation_steps = val_steps
))

model %>% evaluate(test_gen, steps=test_steps)

plot(history)
```

## GRU

```{r}
model <- keras_model_sequential() %>% 
  bidirectional(layer_gru(units = 16, 
            input_shape = list(NULL, dim(chr1)[-1]-1),
            recurrent_activation = "sigmoid",
            use_bias = TRUE,
            reset_after = TRUE,
            return_sequences = TRUE)) %>% 
  bidirectional(layer_gru(units = 8, 
            input_shape = list(NULL, dim(chr1)[-1]-1),
            recurrent_activation = "sigmoid",
            use_bias = TRUE,
            reset_after = TRUE)) %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = "acc"
)

system.time(history <- model %>% fit(
  train_gen,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = val_gen,
  validation_steps = val_steps
))

model %>% evaluate(test_gen, steps=test_steps)

plot(history)
```

## Combining CNNs and RNNs to process long sequences

```{r}
model_C <- keras_model_sequential() %>% 
  layer_conv_1d(filters = 8, kernel_size = 5, activation = "relu",
                input_shape = list(NULL, dim(chr1)[-1]-1)) %>% 
  layer_max_pooling_1d(pool_size = 3) %>% 
  layer_conv_1d(filters = 8, kernel_size = 5, activation = "relu") %>% 
  layer_max_pooling_1d(pool_size = 3) %>% 
  layer_conv_1d(filters = 8, kernel_size = 5, activation = "relu") %>% 
  layer_global_max_pooling_1d() %>% 
  layer_dense(units = 1)


model_C %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = "acc"
)

system.time(history_C <- model_C %>% fit_generator(
  train_gen,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = val_gen,
  validation_steps = val_steps
))

model_C %>% evaluate(test_gen, steps=test_steps)
```


```{r}
plot(history_C)
```

This is the model, starting with two `layer_conv_1d()` and following up with a `layer_gru()`:

```{r}
model_CR <- keras_model_sequential() %>% 
  layer_conv_1d(filters = 8, kernel_size = 5, activation = "relu",
                input_shape = list(NULL, dim(chr1)[-1]-1)) %>% 
  layer_max_pooling_1d(pool_size = 3) %>% 
  layer_conv_1d(filters = 8, kernel_size = 5, activation = "relu") %>% 
  layer_gru(units = 8, dropout = 0.1, recurrent_dropout = 0.5) %>% 
  layer_dense(units = 1)

summary(model_CR)
```

```{r}
model_CR %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = "acc"
)

system.time(history_CR <- model_CR %>% fit_generator(
  train_gen,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = val_gen,
  validation_steps = val_steps
))

model_CR %>% evaluate(test_gen, steps=test_steps)
```

```{r}
plot(history_CR)
```