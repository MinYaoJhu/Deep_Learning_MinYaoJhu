---
title: "From Julin's method"
author: "Min-Yao"
date: "7/21/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(magrittr)
library(Biostrings)
library(GenomicRanges)
library(rtracklayer)
```


## Intro

Want to use ML to predict coding and non coding sequences

First need to figure out how to code each base as coding or not

## download and import files

```{r}
timestamp()
```


```{r}
# if (!dir.exists("AtSeqs")) {
#   dir.create("AtSeqs")
#   download.file("ftp://ftp.arabidopsis.org/home/tair/Maps/gbrowse_data/TAIR10/TAIR10_GFF3_genes.gff", "AtSeqs/TAIR10_GFF3_genes.gff")
#   download.file("ftp://ftp.arabidopsis.org/home/tair/Sequences/whole_chromosomes/TAIR10_chr1.fas", "AtSeqs/TAIR10_chr1.fas")
# }
```

```{r}
# rtracklayer
gff <- import.gff("AtSeqs/TAIR10_GFF3_genes.gff")
gff
```

```{r}
Chr1 <- readDNAStringSet("AtSeqs/TAIR10_chr1.fas")
str(Chr1)
head(Chr1)
```

1 base per row
```{r}
# each cell has one base in it
chr1.tib <- as.matrix(Chr1) %>% t() %>%
  as_tibble() %>%
  rename_with(~ str_remove(.," .*")) %>%
  mutate(pos=1:nrow(.))
head(chr1.tib)
```
create gff for just the CDS
```{r}
gffCDS <- gff[gff$type=="CDS",]
gffCDS
```
### 1) update code to focus on + strand only
query the CDS
```{r}
# First create a query Granges, one per base and + strand only
chr1.query <- GRanges(seqnames = "Chr1", ranges=IRanges(start = chr1.tib$pos, width = 1), strand="+")

chr1.tib$CDS <- findOverlaps(chr1.query, gffCDS, select = "first", ignore.strand=TRUE) %>%
  is.na() %>%
  not() %>% # reverse false and true
  as.integer()
```

```{r}
chr1.tib
```

take a look at the first 10 coding bases:
```{r}
chr1.tib %>% filter(CDS==1) %>%
  extract(1:10,)
```

```{r}
rm(chr1.query, gff, Chr1, gffCDS)
gc()
```

### 2) make one hot encoding for base (or possibly codon)

```{r}
system.time(
  chr1.1_hot <- chr1.tib %>%
    filter(CDS==1) %>%
    pivot_wider(values_from = Chr1, 
                names_from = Chr1, 
                values_fn = function(x) 1L, 
                values_fill = 0L)
)

head(chr1.1_hot)
```
> Y = Cytosine / Thymine (pyrimidine)

```{r}
system.time(write_csv(chr1.1_hot, file="AtSeqs/chr1_1hot.csv.gz"))
```

### 3) think about data structure for having sequence behind and ahead of where prediction is being made.

```{r}
data <- read_csv("AtSeqs/chr1_1hot.csv.gz") %>%
  select(-c(pos,CDS)) %>%
  data.matrix()

head(data)
```

```{r}
generator <- function(data, lookback, delay, min_index, max_index,
                      shuffle = FALSE, batch_size = 16, step = 2) {
  if (is.null(max_index))
    max_index <- nrow(data) - delay - 1
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size-1, max_index))
      i <<- i + length(rows)
    }
    
    samples <- array(0, dim = c(length(rows), 
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
                     
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]], 
                     length.out = dim(samples)[[2]])
      samples[j,,] <- data[indices,]
      targets[[j]] <- data[rows[[j]] + delay,1]
    }            
    
    list(samples, targets)
  }
}
```

```{r}
nrow(data)
lookback <- 200
step <- 1
delay <- 20
batch_size <- 16

train_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 6000000,
  shuffle = TRUE,
  step = step, 
  batch_size = batch_size
)

val_gen = generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 6000001,
  max_index = 7000000,
  step = step,
  batch_size = batch_size
)

test_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 7000001,
  max_index = 8880984,
  step = step,
  batch_size = batch_size
)

# This is how many steps to draw from `val_gen`
# in order to see the whole validation set:
val_steps <- (7000000 - 6000001 - lookback) / batch_size
val_steps

test_steps <- (8880984 - 7000001 - lookback) / batch_size
test_steps
```

```{r}
### notes from class to set up good number for steps per epoch and validation steps

#I am using a batch size of 16, so we should use
6000000/16 # steps per epoch

1000000/16 # validation steps
```

```{r}
library(keras)
```


```{r, echo=TRUE, results='hide'}

model <- keras_model_sequential() %>% 
  layer_flatten(input_shape = c(lookback / step, dim(data)[-1])) %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

system.time(history <- model %>% fit(
  train_gen,
  steps_per_epoch = 200,
  epochs = 10,
  validation_data = val_gen,
  validation_steps = val_steps
))
```





