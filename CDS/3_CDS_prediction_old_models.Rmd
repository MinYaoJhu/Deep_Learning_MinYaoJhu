---
title: "3_CDS_old_model"
author: "Min-Yao"
date: "8/23/2021"
output: html_document
---

```{r}
library(tidyverse)
library(keras)
library(tensorflow)
tf$config$experimental$list_physical_devices()
```

### 3) think about data structure for having sequence behind and ahead of where prediction is being made.

```{r}
data <- read_csv("AtSeqs/chr1_1_hot.csv.gz") %>%
  select(-c(pos)) %>%
  data.matrix()

head(data)
```


```{r}
# generator <- function(data, lookback, delay=0, min_index, max_index,
#                       shuffle = FALSE, batch_size = 16, step = 1) { # half lookback
#   if (is.null(max_index))
#     max_index <- nrow(data) - delay - lookback - 1 # not include training in prediction
#   i <- min_index + lookback
#   function() { # last object that is created will be return by the end of function
#     if (shuffle) {
#       rows <- sample(c((min_index+lookback):max_index), size = batch_size) # batch size run 32 at once # pick bases randomly
#     } else {
#       if (i + batch_size >= max_index)
#         i <<- min_index + lookback
#       rows <- c(i:min(i+batch_size-1, max_index))
#       i <<- i + length(rows) # super assignment -> preserve it in i
#     }
# 
#     samples <- array(0, dim = c(length(rows),
#                                 lookback / step,
#                                 dim(data)[[-1]])) # 4 col ATGC 10 row
#     targets <- array(0, dim = c(length(rows)))
# 
#     for (j in 1:length(rows)) {
#       indices <- seq(rows[[j]] - lookback, rows[[j]], 
#                      length.out = dim(samples)[[2]])
#       samples[j,,] <- data[indices,]
#       targets[[j]] <- data[rows[[j]] + delay,1]
#     }   
# 
#     list(samples, targets)
#   }
# }
```

```{r}
generator <- function(data, lookback, delay=0, min_index, max_index,
                      shuffle = FALSE, batch_size = 32, step = 1) { #note lookback is really half lookback
  if (is.null(max_index))
    max_index <- nrow(data) - delay - lookback - 1
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size-1, max_index))
      i <<- i + length(rows)
    }

    samples <- array(0, dim = c(length(rows),
                                lookback / step,
                                2* (dim(data)[[-1]]-1))) # don't include CDS in samples, do include data from forward and backwards
    targets <- array(0, dim = c(length(rows)))

    for (j in 1:length(rows)) {
      indices.bak <- seq(rows[[j]] - lookback + 1, rows[[j]],  #why do I need +1?  error in original?
                     length.out = dim(samples)[[2]])
      indices.for <- indices.bak+lookback
      # indices.bak go from -lookback to the focal position
      # indices.for for from (1+the focal position) to (1+focal+lookback)

      #now each row of samples has data from before and after the focal base, getting progressively further away
      samples[j,,] <- cbind(data[indices.bak, -1 ], # -1 = don't include CDS in samples
                            data[indices.for, -1 ])

      targets[[j]] <- data[rows[[j]] + delay,1]
    }

    list(samples, targets)
  }
}
```

### 4) make generators

I will use the following parameter values:

* `lookback = 200`, i.e. 200 bp backward and forward
* `steps = 1`, i.e. I will be sampled at every data point.


```{r}
nrow(data)
lookback <- 200
step <- 1
delay <- 0
batch_size <- 16

train_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 5000000,
  shuffle = TRUE,
  step = step, 
  batch_size = batch_size
)

val_gen = generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 5000001,
  max_index = 5100000,
  step = step,
  batch_size = batch_size
)

test_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 6000001,
  max_index = 7000000,
  step = step,
  batch_size = batch_size
)

# This is how many steps to draw from `val_gen`
# in order to see the whole validation set:
val_steps <- (5100000 - 5000001 - lookback) / batch_size
val_steps

test_steps <- (7000000 - 6000001 - lookback) / batch_size
test_steps
```


```{r}
### notes from class to set up good number for steps per epoch and validation steps

#I am using a batch size of 16, so I should use
#6000/16 # steps per epoch

#1000/16 # validation steps
```

```{r}
library(keras)
```

## A basic machine learning approach

```{r, echo=TRUE, results='hide'}

model <- keras_model_sequential() %>% 
  layer_flatten(input_shape = c(lookback / step, 2*(dim(data)[-1]-1))) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = "acc"
)

system.time(history <- model %>% fit(
  train_gen,
  steps_per_epoch = 200,
  epochs = 10,
  validation_data = val_gen,
  validation_steps = val_steps
))

model %>% evaluate(test_gen, steps=test_steps)
```


```{r}
plot(history)
```

> about 32 minutes, val_acc: 0.8393

## A first recurrent baseline

```{r}
model_r <- keras_model_sequential() %>% 
  layer_gru(units = 16, input_shape = list(NULL, 2*(dim(data)[-1]-1))) %>% 
  layer_dense(units = 1)

model_r %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = "acc"
)

system.time(history_r <- model_r %>% fit_generator(
  train_gen,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = val_gen,
  validation_steps = val_steps
))

model_r %>% evaluate(test_gen, steps=test_steps)
```



Let look at our results:

```{r}
plot(history_r)
```

> about 62 minutes, val_acc: 0.8019


## Using recurrent dropout to fight overfitting

```{r}
model_rd <- keras_model_sequential() %>% 
  layer_gru(units = 16, dropout = 0.2, recurrent_dropout = 0.2,
            input_shape = list(NULL, 2*(dim(data)[-1]-1))) %>% 
  layer_dense(units = 1)

model_rd %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = "acc"
)

system.time(history_rd <- model_rd %>% fit_generator(
  train_gen,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = val_gen,
  validation_steps = val_steps
))

model_rd %>% evaluate(test_gen, steps=test_steps)
```

```{r}
plot(history_rd)
```

> about 51 minutes

## Stacking recurrent layers

```{r}
model_sr <- keras_model_sequential() %>% 
  layer_gru(units = 16, 
            dropout = 0.1, 
            recurrent_dropout = 0.5,
            return_sequences = TRUE,
            input_shape = list(NULL, 2*(dim(data)[-1]-1))) %>% 
  layer_gru(units = 16, activation = "relu",
            dropout = 0.1,
            recurrent_dropout = 0.5) %>% 
  layer_dense(units = 1)

model_sr %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = "acc"
)

system.time(history_sr <- model_sr %>% fit_generator(
  train_gen,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = val_gen,
  validation_steps = val_steps
))

model_sr %>% evaluate(test_gen, steps=test_steps)
```

```{r}
plot(history_sr)
```

> about 1 hr 13 minutes, val_acc: 0.6294

## Using bidirectional RNNs

```{r}
model_bR <- keras_model_sequential() %>% 
  bidirectional(
    layer_gru(units = 16), input_shape = list(NULL, 2*(dim(data)[-1]-1))
  ) %>% 
  layer_dense(units = 1)

model_bR %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = "acc"
)

system.time(history_bR <- model_bR %>% fit_generator(
  train_gen,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = val_gen,
  validation_steps = val_steps
))

model_bR %>% evaluate(test_gen, steps=test_steps)
```

```{r}
plot(history_bR)
```

> about 2.1 hours, val_acc: 0.8368

## Combining CNNs and RNNs to process long sequences

```{r}
model_C <- keras_model_sequential() %>% 
  layer_conv_1d(filters = 16, kernel_size = 5, activation = "relu",
                input_shape = list(NULL, 2*(dim(data)[-1]-1))) %>% 
  layer_max_pooling_1d(pool_size = 3) %>% 
  layer_conv_1d(filters = 16, kernel_size = 5, activation = "relu") %>% 
  layer_max_pooling_1d(pool_size = 3) %>% 
  layer_conv_1d(filters = 16, kernel_size = 5, activation = "relu") %>% 
  layer_global_max_pooling_1d() %>% 
  layer_dense(units = 1)


model_C %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = "acc"
)

system.time(history_C <- model_C %>% fit_generator(
  train_gen,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = val_gen,
  validation_steps = val_steps
))

model_C %>% evaluate(test_gen, steps=test_steps)
```

```{r}
plot(history_C)
```

> about 35 minutes, val_acc: 0.7572

This is the model, starting with two `layer_conv_1d()` and following up with a `layer_gru()`:

```{r}
model_CR <- keras_model_sequential() %>% 
  layer_conv_1d(filters = 16, kernel_size = 5, activation = "relu",
                input_shape = list(NULL, 2*(dim(data)[-1]-1))) %>% 
  layer_max_pooling_1d(pool_size = 3) %>% 
  layer_conv_1d(filters = 16, kernel_size = 5, activation = "relu") %>% 
  layer_gru(units = 16, dropout = 0.1, recurrent_dropout = 0.5) %>% 
  layer_dense(units = 1)

summary(model_CR)
```

```{r}
model_CR %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = "acc"
)

system.time(history_CR <- model_CR %>% fit_generator(
  train_gen,
  steps_per_epoch = 100,
  epochs = 10,
  validation_data = val_gen,
  validation_steps = val_steps
))

model_CR %>% evaluate(test_gen, steps=test_steps)
```

```{r}
plot(history_CR)
```

> about 60 minutes, val_acc: 0.7558
