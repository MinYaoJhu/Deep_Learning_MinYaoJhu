---
title: "OJ"
author: "Min-Yao"
date: "2021/1/16"
output: 
  html_document: 
    keep_md: yes
---

## OJ

```{r}
library(tidyverse)
library(ISLR)
library(tree)
data(OJ)
#?OJ
```

```{r}
head(OJ)
summary(OJ)
```

```{r}
OJ %>% 
  select(StoreID, STORE) %>% 
  head()
```
> StoreID and STORE are redundant
> because store is categorical we need to turn that into a series of dummy variables.

```{r}
store_cat <- OJ %>% 
  select(StoreID) %>%
  mutate(row=1:nrow(.),data=1) %>%
  pivot_wider(id_cols = row, names_from=StoreID, values_from=data, values_fill=0, names_prefix="Store")
head(store_cat)
```

```{r}
OJ <- OJ %>% 
  select(-StoreID, -STORE, -Store7) %>% 
  cbind(store_cat) %>% 
  select(-row)
head(OJ)
```

## (a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.

```{r}
set.seed(10)
train <- sample(1:nrow(OJ), size = 800)
OJ.train <- OJ[train,]
OJ.test <- OJ[-train,]
str(OJ.test)
str(OJ.train)
```
## (b) Fit a tree to the training data, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics about the tree, and describe the results obtained. What is the training error rate? How many terminal nodes does the tree have?

```{r}
tree.OJ=tree(Purchase~.,OJ,subset=train)
summary(tree.OJ)
```
> Number of terminal nodes:  7

> Misclassification error rate: 0.1775

## (e) Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?

```{r}
tree.pred=predict(tree.OJ,OJ.test,type="class")
purchase.test=OJ.test$Purchase
table(tree.pred,purchase.test)
(135+88)/270
1-((135+88)/270)
```

> Accuracy is 82.59%

## Deep learning method

```{r}
library(keras)
library(reticulate)
library(tensorflow)
use_condaenv("r-reticulate")
```

> prepare label

```{r}
OJ.train.label <- OJ.train %>% select(Purchase) %>% mutate(Purchase=ifelse(Purchase=="CH", 0, 1)) %>% pull(Purchase) # pull take it out as a vector, the same as oj[["purchase"]]
str(OJ.train.label)
OJ.test.label <- OJ.test %>% select(Purchase) %>% mutate(Purchase=ifelse(Purchase=="CH", 0, 1)) %>% pull(Purchase)
str(OJ.test.label)
```

> prepare data

```{r}
OJ.train <- OJ.train %>% select(-Purchase)
str(OJ.train)
OJ.test <- OJ.test %>% select(-Purchase)
str(OJ.test)
```

# add missing data as 0

```{r}
oj.missing <- OJ.train %>% as.matrix() # must be a matrix for this to work
sum(oj.missing==0) # how many zeros are there already?
dim(oj.missing) 
length(oj.missing) # matrixes are really vectors that flow into a 2D space

# put 0 in 10% of the cells
oj.missing[sample(length(oj.missing), size = 0.1*length(oj.missing))] <- 0
sum(oj.missing==0)
dim(oj.missing)
length(oj.missing)
```

> scale data

```{r}
oj.mean <- apply(OJ.train, 2, mean)
oj.std <- apply(OJ.train, 2, sd)

OJ.train <- scale(OJ.train, center=oj.mean, scale=oj.std)
OJ.test <- scale(OJ.test, center=oj.mean, scale=oj.std)

str(OJ.train)
str(OJ.test)
```

> Setting aside a validation set

```{r}
set.seed(10)
val_indices <- sample(1:nrow(OJ.train), size = 100)

OJ.train_val <- OJ.train[val_indices,]
partial_OJ.train <- OJ.train[-val_indices,]

# class(OJ.train) # to know the 

# OJ.test <- data.matrix(OJ.test, rownames.force = NA)
# OJ.train <- data.matrix(OJ.train, rownames.force = NA)
# OJ.train_val <- data.matrix(OJ.train_val, rownames.force = NA)
# partial_OJ.train <- data.matrix(partial_OJ.train, rownames.force = NA)

# matrix and dataframe are different
# matrix need to be the same element (as )

OJ.train_val.label <- OJ.train.label[val_indices]
partial_OJ.train.label <- OJ.train.label[-val_indices]

dim(OJ.train_val)
dim(partial_OJ.train)

class(OJ.train_val)
class(partial_OJ.train)
class(OJ.train_val.label)
class(partial_OJ.train.label)

str(OJ.train_val)
str(partial_OJ.train)
str(OJ.train_val.label)
str(partial_OJ.train.label)

```

### original model: use units = 16 and 2 layers to run the model.

```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = ncol(OJ.train) ) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")


model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

system.time(history <- model %>% fit(
  partial_OJ.train,
  partial_OJ.train.label,
  epochs = 100,
  batch_size = 256,
  validation_data = list(OJ.train_val, OJ.train_val.label),
  verbose = 0
))
```

```{r}
str(history)
```

```{r}
plot(history)
```

> I choose 60, run with 60 epochs and full training:

```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = ncol(OJ.train) ) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")


model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

system.time(history <- model %>% fit(
  OJ.train,
  OJ.train.label,
  epochs = 60,
  batch_size = 256,
  verbose = 0
))
```


```{r}
plot(history)
```


```{r}
results <- model %>% evaluate(OJ.test, OJ.test.label)

results
```

> Accuracy is 85.55%, which is a little bit better than the trees (82.59%).


### batch size also need to be power of 2
### smaller train batch size is better, but also slower

# missing data version

> scale data with missing

```{r}
oj.mean <- apply(oj.missing, 2, mean)
oj.std <- apply(oj.missing, 2, sd)

OJ.train <- scale(oj.missing, center=oj.mean, scale=oj.std)
OJ.test <- scale(OJ.test, center=oj.mean, scale=oj.std)

str(OJ.train)
str(OJ.test)
```

> Setting aside a validation set

```{r}
set.seed(10)
val_indices <- sample(1:nrow(OJ.train), size = 100)

OJ.train_val <- OJ.train[val_indices,]
partial_OJ.train <- OJ.train[-val_indices,]

# class(OJ.train) # to know the 

# OJ.test <- data.matrix(OJ.test, rownames.force = NA)
# OJ.train <- data.matrix(OJ.train, rownames.force = NA)
# OJ.train_val <- data.matrix(OJ.train_val, rownames.force = NA)
# partial_OJ.train <- data.matrix(partial_OJ.train, rownames.force = NA)

# matrix and dataframe are different
# matrix need to be the same element (as )

OJ.train_val.label <- OJ.train.label[val_indices]
partial_OJ.train.label <- OJ.train.label[-val_indices]

dim(OJ.train_val)
dim(partial_OJ.train)

class(OJ.train_val)
class(partial_OJ.train)
class(OJ.train_val.label)
class(partial_OJ.train.label)

str(OJ.train_val)
str(partial_OJ.train)
str(OJ.train_val.label)
str(partial_OJ.train.label)

```

### original model: use units = 16 and 2 layers to run the model.

```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = ncol(OJ.train) ) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")


model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

system.time(history <- model %>% fit(
  partial_OJ.train,
  partial_OJ.train.label,
  epochs = 100,
  batch_size = 256,
  validation_data = list(OJ.train_val, OJ.train_val.label),
  verbose = 0
))
```

```{r}
str(history)
```

```{r}
plot(history)
```

> I choose 40, run with 40 epochs and full training:

```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = ncol(OJ.train) ) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")


model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

system.time(history <- model %>% fit(
  OJ.train,
  OJ.train.label,
  epochs = 40,
  batch_size = 256,
  verbose = 0
))
```


```{r}
plot(history)
```


```{r}
results <- model %>% evaluate(OJ.test, OJ.test.label)

results
```

> Accuracy is 77.03%, which is worse than the original model (85.55%).

> it seems like adding 0 before scaling does influence the model traning process

