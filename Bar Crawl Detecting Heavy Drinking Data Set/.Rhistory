plot(history)
model_r <- keras_model_sequential() %>%
layer_gru(units = 16, input_shape = c(lookback / step, 3)) %>%
layer_dense(units = 1)
model_r %>% compile(
optimizer = optimizer_rmsprop(),
loss = "binary_crossentropy",
metrics = "accuracy"
)
system.time(history_r <- model_r %>% fit_generator(
train_gen,
steps_per_epoch = 50,
epochs = 10,
validation_data = val_gen,
validation_steps = val_steps
))
model_r %>% evaluate(test_gen, steps=test_steps)
plot(history_r)
model_rd <- keras_model_sequential() %>%
layer_gru(units = 16, dropout = 0.2, recurrent_dropout = 0.2,
input_shape = c(lookback / step, 3)) %>%
layer_dense(units = 1)
model_rd %>% compile(
optimizer = optimizer_rmsprop(),
loss = "binary_crossentropy",
metrics = "accuracy"
)
system.time(history_rd <- model_rd %>% fit_generator(
train_gen,
steps_per_epoch = 50,
epochs = 10,
validation_data = val_gen,
validation_steps = val_steps
))
model_rd %>% evaluate(test_gen, steps=test_steps)
plot(history_rd)
model_C <- keras_model_sequential() %>%
layer_conv_1d(filters = 16, kernel_size = 5, activation = "relu",
input_shape = c(lookback / step, 3)) %>%
layer_max_pooling_1d(pool_size = 3) %>%
layer_conv_1d(filters = 16, kernel_size = 5, activation = "relu") %>%
layer_max_pooling_1d(pool_size = 3) %>%
layer_conv_1d(filters = 16, kernel_size = 5, activation = "relu") %>%
layer_global_max_pooling_1d() %>%
layer_dense(units = 1)
model_C %>% compile(
optimizer = optimizer_rmsprop(),
loss = "binary_crossentropy",
metrics = "acc"
)
system.time(history_C <- model_C %>% fit_generator(
train_gen,
steps_per_epoch = 100,
epochs = 10,
validation_data = val_gen,
validation_steps = val_steps
))
model_C %>% evaluate(test_gen, steps=test_steps)
plot(history_C)
library(tidyverse)
library(keras)
use_condaenv("r-reticulate")
acc <- read_csv("all_accelerometer_data_pids_13.csv")
head(acc)
str(acc)
acc_a <- acc %>% mutate(time_30m = as.integer(round(time/1000/60/30)))
acc_a
str(acc_a)
data_files <- list.files("clean_tac/",full.names = TRUE)
data_list <- setNames(lapply(data_files, read_csv), data_files)
tac_all <- bind_rows(data_list,.id = "id")
tac_all
tac_all %>% mutate(realtime=as.POSIXct(timestamp, origin="1970-01-01"))
tac_all %>% mutate(realtime=as.POSIXct(timestamp, origin="1970-01-01")) %>%
mutate(pid=str_remove(id, "clean_tac/")) %>%
mutate(pid=str_remove(pid, "_clean_TAC.csv")) %>%
ggplot(aes(x=realtime, y=TAC_Reading, color=pid)) +
geom_point()
tac_all %>% mutate(realtime=as.POSIXct(timestamp, origin="1970-01-01")) %>%
mutate(pid=str_remove(id, "clean_tac/")) %>%
mutate(pid=str_remove(pid, "_clean_TAC.csv")) %>%
ggplot(aes(x=realtime, y=TAC_Reading, color=pid)) +
facet_wrap(~pid, ncol=4) +
geom_point()
tac_all %>% mutate(realtime=as.POSIXct(timestamp, origin="1970-01-01")) %>%
mutate(pid=str_remove(id, "clean_tac/")) %>%
mutate(pid=str_remove(pid, "_clean_TAC.csv")) %>%
ggplot(aes(x=realtime, y=pid, color=pid)) +
geom_point()
tac_all_a <- tac_all %>%
mutate(pid=str_remove(id, "clean_tac/")) %>%
mutate(pid=str_remove(pid, "_clean_TAC.csv")) %>%
mutate(time_30m=as.integer(timestamp/60/30)) %>%
select(pid, timestamp, time_30m, TAC_Reading) %>%
arrange(pid, timestamp)
tac_all_a
data <- inner_join(tac_all_a, acc_a) %>%
mutate(intox=(TAC_Reading > 0.08) * 1L)
data
generator <- function(data, lookback, delay, min_index, max_index,
shuffle = FALSE, batch_size = 16, step = 1) {
if (is.null(max_index))
max_index <- nrow(data) - delay
i <- min_index + lookback
function() {
if (shuffle) {
rows <- sample(c((min_index+lookback):max_index), size = batch_size)
} else {
if (i + batch_size >= max_index)
i <<- min_index + lookback
rows <- c(i:min(i+batch_size, max_index))
i <<- i + length(rows)
}
samples <- array(0, dim = c(length(rows),
lookback / step,
3))
targets <- array(0, dim = c(length(rows)))
for (j in 1:length(rows)) {
indices <- seq(rows[[j]] - lookback, rows[[j]],
length.out = dim(samples)[[2]])
# make sure all from same person
discord <- sum(data$pid[indices] != data$pid[rows[[j]]])
if (discord > 0) {
indices <- indices + discord
rows[[j]] + discord
}
samples[j,,] <- as.matrix(data[round(indices),c("x", "y", "z")])
targets[[j]] <- as.matrix(data[rows[[j]] + delay, "intox"])
}
list(samples, targets)
}
}
lookback <- 3600
step <- 4
delay <- 0
batch_size <- 16
train_gen <- generator(
data = data,
lookback = lookback,
delay = delay,
min_index = 1,
max_index = {which(data$pid=="MC7070") %>% max()},
shuffle = TRUE,
step = step,
batch_size = batch_size
)
val_gen = generator(
data = data,
lookback = lookback,
delay = delay,
min_index = {which(data$pid=="MC7070") %>% max() + 1},
max_index = {which(data$pid=="PC6771") %>% max()},
shuffle = TRUE,
step = step,
batch_size = batch_size
)
test_gen <- generator(
data = data,
lookback = lookback,
delay = delay,
min_index = {which(data$pid=="PC6771") %>% max() + 1},
max_index = nrow(data),
shuffle = TRUE,
step = step,
batch_size = batch_size
)
# This is how many steps to draw from `val_gen`
# in order to see the whole validation set:
val_steps <- ({which(data$pid=="MJ8002") %>% max()} - {which(data$pid=="MC7070") %>% max() + 1} - lookback) / batch_size
val_steps
test_steps <- (nrow(data) - {which(data$pid=="PC6771") %>% max() + 1} - lookback) / batch_size
test_steps
val_steps <- 1600/batch_size
val_steps
test_steps <- 1600/batch_size
test_steps
set.seed(1)
model <- keras_model_sequential() %>%
layer_flatten(input_shape = c(lookback / step, 3)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)
model %>% compile(
optimizer = optimizer_rmsprop(),
loss = "binary_crossentropy",
metrics = "accuracy"
)
system.time(history <- model %>% fit(
train_gen,
steps_per_epoch = 100,
epochs = 20,
validation_data = val_gen,
validation_steps = val_steps
))
model %>% evaluate(test_gen, steps=test_steps)
plot(history)
model_r <- keras_model_sequential() %>%
layer_gru(units = 16, input_shape = c(lookback / step, 3)) %>%
layer_dense(units = 1)
model_r %>% compile(
optimizer = optimizer_rmsprop(),
loss = "binary_crossentropy",
metrics = "accuracy"
)
system.time(history_r <- model_r %>% fit_generator(
train_gen,
steps_per_epoch = 100,
epochs = 20,
validation_data = val_gen,
validation_steps = val_steps
))
model_r %>% evaluate(test_gen, steps=test_steps)
plot(history_r)
model_rd <- keras_model_sequential() %>%
layer_gru(units = 16, dropout = 0.2, recurrent_dropout = 0.2,
input_shape = c(lookback / step, 3)) %>%
layer_dense(units = 1)
model_rd %>% compile(
optimizer = optimizer_rmsprop(),
loss = "binary_crossentropy",
metrics = "accuracy"
)
system.time(history_rd <- model_rd %>% fit_generator(
train_gen,
steps_per_epoch = 100,
epochs = 20,
validation_data = val_gen,
validation_steps = val_steps
))
model_rd %>% evaluate(test_gen, steps=test_steps)
plot(history_rd)
model_C <- keras_model_sequential() %>%
layer_conv_1d(filters = 16, kernel_size = 5, activation = "relu",
input_shape = c(lookback / step, 3)) %>%
layer_max_pooling_1d(pool_size = 3) %>%
layer_conv_1d(filters = 16, kernel_size = 5, activation = "relu") %>%
layer_max_pooling_1d(pool_size = 3) %>%
layer_conv_1d(filters = 16, kernel_size = 5, activation = "relu") %>%
layer_global_max_pooling_1d() %>%
layer_dense(units = 1)
model_C %>% compile(
optimizer = optimizer_rmsprop(),
loss = "binary_crossentropy",
metrics = "acc"
)
system.time(history_C <- model_C %>% fit_generator(
train_gen,
steps_per_epoch = 100,
epochs = 20,
validation_data = val_gen,
validation_steps = val_steps
))
model_C %>% evaluate(test_gen, steps=test_steps)
plot(history_C)
library(tidyverse)
library(keras)
use_condaenv("r-reticulate")
acc <- read_csv("all_accelerometer_data_pids_13.csv")
head(acc)
str(acc)
acc_a <- acc %>% mutate(time_30m = as.integer(round(time/1000/60/30)))
acc_a
str(acc_a)
data_files <- list.files("clean_tac/",full.names = TRUE)
data_list <- setNames(lapply(data_files, read_csv), data_files)
tac_all <- bind_rows(data_list,.id = "id")
tac_all
tac_all %>% mutate(realtime=as.POSIXct(timestamp, origin="1970-01-01"))
tac_all %>% mutate(realtime=as.POSIXct(timestamp, origin="1970-01-01")) %>%
mutate(pid=str_remove(id, "clean_tac/")) %>%
mutate(pid=str_remove(pid, "_clean_TAC.csv")) %>%
ggplot(aes(x=realtime, y=TAC_Reading, color=pid)) +
geom_point()
tac_all %>% mutate(realtime=as.POSIXct(timestamp, origin="1970-01-01")) %>%
mutate(pid=str_remove(id, "clean_tac/")) %>%
mutate(pid=str_remove(pid, "_clean_TAC.csv")) %>%
ggplot(aes(x=realtime, y=TAC_Reading, color=pid)) +
facet_wrap(~pid, ncol=4) +
geom_point()
tac_all %>% mutate(realtime=as.POSIXct(timestamp, origin="1970-01-01")) %>%
mutate(pid=str_remove(id, "clean_tac/")) %>%
mutate(pid=str_remove(pid, "_clean_TAC.csv")) %>%
ggplot(aes(x=realtime, y=pid, color=pid)) +
geom_point()
tac_all_a <- tac_all %>%
mutate(pid=str_remove(id, "clean_tac/")) %>%
mutate(pid=str_remove(pid, "_clean_TAC.csv")) %>%
mutate(time_30m=as.integer(timestamp/60/30)) %>%
select(pid, timestamp, time_30m, TAC_Reading) %>%
arrange(pid, timestamp)
tac_all_a
data <- inner_join(tac_all_a, acc_a) %>%
mutate(intox=(TAC_Reading > 0.08) * 1L)
data
generator <- function(data, lookback, delay, min_index, max_index,
shuffle = FALSE, batch_size = 16, step = 1) {
if (is.null(max_index))
max_index <- nrow(data) - delay
i <- min_index + lookback
function() {
if (shuffle) {
rows <- sample(c((min_index+lookback):max_index), size = batch_size)
} else {
if (i + batch_size >= max_index)
i <<- min_index + lookback
rows <- c(i:min(i+batch_size, max_index))
i <<- i + length(rows)
}
samples <- array(0, dim = c(length(rows),
lookback / step,
3))
targets <- array(0, dim = c(length(rows)))
for (j in 1:length(rows)) {
indices <- seq(rows[[j]] - lookback, rows[[j]],
length.out = dim(samples)[[2]])
# make sure all from same person
discord <- sum(data$pid[indices] != data$pid[rows[[j]]])
if (discord > 0) {
indices <- indices + discord
rows[[j]] + discord
}
samples[j,,] <- as.matrix(data[round(indices),c("x", "y", "z")])
targets[[j]] <- as.matrix(data[rows[[j]] + delay, "intox"])
}
list(samples, targets)
}
}
lookback <- 6000
step <- 4
delay <- 0
batch_size <- 32
train_gen <- generator(
data = data,
lookback = lookback,
delay = delay,
min_index = 1,
max_index = {which(data$pid=="MC7070") %>% max()},
shuffle = TRUE,
step = step,
batch_size = batch_size
)
val_gen = generator(
data = data,
lookback = lookback,
delay = delay,
min_index = {which(data$pid=="MC7070") %>% max() + 1},
max_index = {which(data$pid=="PC6771") %>% max()},
shuffle = TRUE,
step = step,
batch_size = batch_size
)
test_gen <- generator(
data = data,
lookback = lookback,
delay = delay,
min_index = {which(data$pid=="PC6771") %>% max() + 1},
max_index = nrow(data),
shuffle = TRUE,
step = step,
batch_size = batch_size
)
# This is how many steps to draw from `val_gen`
# in order to see the whole validation set:
val_steps <- ({which(data$pid=="MJ8002") %>% max()} - {which(data$pid=="MC7070") %>% max() + 1} - lookback) / batch_size
val_steps
test_steps <- (nrow(data) - {which(data$pid=="PC6771") %>% max() + 1} - lookback) / batch_size
test_steps
val_steps <- 6000/batch_size
val_steps
test_steps <- 6000/batch_size
test_steps
set.seed(1)
model <- keras_model_sequential() %>%
layer_flatten(input_shape = c(lookback / step, 3)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)
model %>% compile(
optimizer = optimizer_rmsprop(),
loss = "binary_crossentropy",
metrics = "accuracy"
)
system.time(history <- model %>% fit(
train_gen,
steps_per_epoch = 100,
epochs = 20,
validation_data = val_gen,
validation_steps = val_steps
))
model %>% evaluate(test_gen, steps=test_steps)
plot(history)
model_r <- keras_model_sequential() %>%
layer_gru(units = 16, input_shape = c(lookback / step, 3)) %>%
layer_dense(units = 1)
model_r %>% compile(
optimizer = optimizer_rmsprop(),
loss = "binary_crossentropy",
metrics = "accuracy"
)
system.time(history_r <- model_r %>% fit_generator(
train_gen,
steps_per_epoch = 100,
epochs = 20,
validation_data = val_gen,
validation_steps = val_steps
))
model_r %>% evaluate(test_gen, steps=test_steps)
plot(history_r)
model_rd <- keras_model_sequential() %>%
layer_gru(units = 16, dropout = 0.2, recurrent_dropout = 0.2,
input_shape = c(lookback / step, 3)) %>%
layer_dense(units = 1)
model_rd %>% compile(
optimizer = optimizer_rmsprop(),
loss = "binary_crossentropy",
metrics = "accuracy"
)
system.time(history_rd <- model_rd %>% fit_generator(
train_gen,
steps_per_epoch = 100,
epochs = 20,
validation_data = val_gen,
validation_steps = val_steps
))
model_rd %>% evaluate(test_gen, steps=test_steps)
plot(history_rd)
model_sr <- keras_model_sequential() %>%
layer_gru(units = 16,
dropout = 0.1,
recurrent_dropout = 0.5,
return_sequences = TRUE,
input_shape = c(lookback / step, 3)) %>%
layer_gru(units = 16, activation = "relu",
dropout = 0.1,
recurrent_dropout = 0.5) %>%
layer_dense(units = 1)
model_sr %>% compile(
optimizer = optimizer_rmsprop(),
loss = "binary_crossentropy",
metrics = "accuracy"
)
system.time(history_sr <- model_sr %>% fit_generator(
train_gen,
steps_per_epoch = 100,
epochs = 20,
validation_data = val_gen,
validation_steps = val_steps
))
model_sr %>% evaluate(test_gen, steps=test_steps)
plot(history_sr)
model_bR <- keras_model_sequential() %>%
bidirectional(
layer_gru(units = 16), input_shape = c(lookback / step, 3)
) %>%
layer_dense(units = 1)
model_bR %>% compile(
optimizer = optimizer_rmsprop(),
loss = "binary_crossentropy",
metrics = "accuracy"
)
system.time(history_bR <- model_bR %>% fit_generator(
train_gen,
steps_per_epoch = 100,
epochs = 20,
validation_data = val_gen,
validation_steps = val_steps
))
model_bR %>% evaluate(test_gen, steps=test_steps)
plot(history_bR)
model_C <- keras_model_sequential() %>%
layer_conv_1d(filters = 16, kernel_size = 5, activation = "relu",
input_shape = c(lookback / step, 3)) %>%
layer_max_pooling_1d(pool_size = 3) %>%
layer_conv_1d(filters = 16, kernel_size = 5, activation = "relu") %>%
layer_max_pooling_1d(pool_size = 3) %>%
layer_conv_1d(filters = 16, kernel_size = 5, activation = "relu") %>%
layer_global_max_pooling_1d() %>%
layer_dense(units = 1)
model_C %>% compile(
optimizer = optimizer_rmsprop(),
loss = "binary_crossentropy",
metrics = "acc"
)
system.time(history_C <- model_C %>% fit_generator(
train_gen,
steps_per_epoch = 100,
epochs = 20,
validation_data = val_gen,
validation_steps = val_steps
))
model_C %>% evaluate(test_gen, steps=test_steps)
plot(history_C)
model_CR <- keras_model_sequential() %>%
layer_conv_1d(filters = 16, kernel_size = 5, activation = "relu",
input_shape = c(lookback / step, 3)) %>%
layer_max_pooling_1d(pool_size = 3) %>%
layer_conv_1d(filters = 16, kernel_size = 5, activation = "relu") %>%
layer_gru(units = 16, dropout = 0.1, recurrent_dropout = 0.5) %>%
layer_dense(units = 1)
summary(model_CR)
model_CR %>% compile(
optimizer = optimizer_rmsprop(),
loss = "binary_crossentropy",
metrics = "acc"
)
system.time(history_CR <- model_CR %>% fit_generator(
train_gen,
steps_per_epoch = 100,
epochs = 20,
validation_data = val_gen,
validation_steps = val_steps
))
model_CR %>% evaluate(test_gen, steps=test_steps)
plot(history_CR)
