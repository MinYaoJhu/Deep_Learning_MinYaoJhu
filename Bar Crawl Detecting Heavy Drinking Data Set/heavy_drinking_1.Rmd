---
title: "Heavy_Drinking_1"
author: "Min-Yao"
date: "9/26/2021"
output: html_document
---


```{r}
library(tidyverse)
library(keras)
use_condaenv("r-reticulate")
```

# Bar Crawl: Detecting Heavy Drinking Data Set

Data Set Information:

Relevant Information:
All data is fully anonymized.

Data was originally collected from 19 participants, but the TAC readings of 6 participants were deemed unusable by SCRAM [1]. The data included is from the remaining 13 participants.

Accelerometer data was collected from smartphones at a sampling rate of 40Hz (file: all_accelerometer_data_pids_13.csv). The file contains 5 columns: a timestamp, a participant ID, and a sample from each axis of the accelerometer. Data was collected from a mix of 11 iPhones and 2 Android phones as noted in phone_types.csv. TAC data was collected using SCRAM [2] ankle bracelets and was collected at 30 minute intervals. The raw TAC readings are in the raw_tac directory. TAC readings which are more readily usable for processing are in clean_tac directory and have two columns: a timestamp and TAC reading. The cleaned TAC readings: (1) were processed with a zero-phase low-pass filter to smooth noise without shifting phase; (2) were shifted backwards by 45 minutes so the labels more closely match the true intoxication of the participant (since alcohol takes about 45 minutes to exit through the skin.) Please see the above referenced study for more details on how the data was processed ([Web Link]).

1 - [Web Link]
2 - J. Robert Zettl. The determination of blood alcohol concentration by transdermal measurement. [Web Link], 2002.

Number of Instances:
Accelerometer readings: 14,057,567
TAC readings: 715
Participants: 13

Number of Attributes:
- Time series: 3 axes of accelerometer data (columns x, y, z in all_accelerometer_data_pids_13.csv)
- Static: 1 phone-type feature (in phone_types.csv)
- Target: 1 time series of TAC for each of the 13 participants (in clean_tac directory).

For Each Attribute:
(Main)
all_accelerometer_data_pids_13.csv:
time: integer, unix timestamp, milliseconds
pid: symbolic, 13 categories listed in pids.txt
x: continuous, time-series
y: continuous, time-series
z: continuous, time-series
clean_tac/*.csv:
timestamp: integer, unix timestamp, seconds
TAC_Reading: continuous, time-series
phone_type.csv:
pid: symbolic, 13 categories listed in pids.txt
phonetype: symbolic, 2 categories (iPhone, Android)

(Other)
raw/*.xlsx:
TAC Level: continuous, time-series
IR Voltage: continuous, time-series
Temperature: continuous, time-series
Time: datetime
Date: datetime

Missing Attribute Values:
None

Target Distribution:
TAC is measured in g/dl where 0.08 is the legal limit for intoxication while driving
Mean TAC: 0.065 +/- 0.182
Max TAC: 0.443
TAC Inner Quartiles: 0.002, 0.029, 0.092
Mean Time-to-last-drink: 16.1 +/- 6.9 hrs

## Get the data

### acceleromator

```{r}
acc <- read_csv("all_accelerometer_data_pids_13.csv")
```

```{r}
head(acc)
str(acc)
```
```{r}
acc_a <- acc %>% mutate(time_30m = as.integer(round(time/1000/60/30)))
acc_a
str(acc_a)
```

### TAC

```{r}
data_files <- list.files("clean_tac/",full.names = TRUE)
data_list <- setNames(lapply(data_files, read_csv), data_files)
tac_all <- bind_rows(data_list,.id = "id")
tac_all
```

```{r}
tac_all %>% mutate(realtime=as.POSIXct(timestamp, origin="1970-01-01"))
```
```{r}
tac_all %>% mutate(realtime=as.POSIXct(timestamp, origin="1970-01-01")) %>%
  mutate(pid=str_remove(id, "clean_tac/")) %>% 
  mutate(pid=str_remove(pid, "_clean_TAC.csv")) %>% 
  ggplot(aes(x=realtime, y=TAC_Reading, color=pid)) +
  geom_point()
  
```

```{r}
tac_all %>% mutate(realtime=as.POSIXct(timestamp, origin="1970-01-01")) %>%
  mutate(pid=str_remove(id, "clean_tac/")) %>% 
  mutate(pid=str_remove(pid, "_clean_TAC.csv")) %>% 
  ggplot(aes(x=realtime, y=TAC_Reading, color=pid)) +
  facet_wrap(~pid, ncol=4) +
  geom_point()
```


```{r}
tac_all %>% mutate(realtime=as.POSIXct(timestamp, origin="1970-01-01")) %>%
  mutate(pid=str_remove(id, "clean_tac/")) %>% 
  mutate(pid=str_remove(pid, "_clean_TAC.csv")) %>% 
  ggplot(aes(x=realtime, y=pid, color=pid)) +
  geom_point()
```

```{r}
tac_all_a <- tac_all %>% 
  mutate(pid=str_remove(id, "clean_tac/")) %>% 
  mutate(pid=str_remove(pid, "_clean_TAC.csv")) %>% 
  mutate(time_30m=as.integer(timestamp/60/30)) %>% 
  select(pid, timestamp, time_30m, TAC_Reading) %>%
  arrange(pid, timestamp)
tac_all_a              

```
### combine

```{r}
data <- inner_join(tac_all_a, acc_a) %>%
  mutate(intox=(TAC_Reading > 0.08) * 1L)
data
```

### Generator

```{r}
generator <- function(data, lookback, delay, min_index, max_index,
                      shuffle = FALSE, batch_size = 16, step = 1) {
  if (is.null(max_index))
    max_index <- nrow(data) - delay
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size, max_index))
      i <<- i + length(rows)
    }
    
    samples <- array(0, dim = c(length(rows), 
                                lookback / step,
                                3))
    targets <- array(0, dim = c(length(rows)))
                     
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]], 
                     length.out = dim(samples)[[2]])
      
            # make sure all from same person
      discord <- sum(data$pid[indices] != data$pid[rows[[j]]])
      if (discord > 0) {
        indices <- indices + discord
        rows[[j]] + discord
      }
      
      samples[j,,] <- as.matrix(data[round(indices),c("x", "y", "z")])
      targets[[j]] <- as.matrix(data[rows[[j]] + delay, "intox"])
    }            
    
    list(samples, targets)
  }
}
```

```{r}
lookback <- 6000
step <- 4
delay <- 0
batch_size <- 32

train_gen <- generator(
  data = data,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = {which(data$pid=="MC7070") %>% max()},
  shuffle = TRUE,
  step = step, 
  batch_size = batch_size
)

val_gen = generator(
  data = data,
  lookback = lookback,
  delay = delay,
  min_index = {which(data$pid=="MC7070") %>% max() + 1},
  max_index = {which(data$pid=="PC6771") %>% max()},
  shuffle = TRUE,
  step = step,
  batch_size = batch_size
)

test_gen <- generator(
  data = data,
  lookback = lookback,
  delay = delay,
  min_index = {which(data$pid=="PC6771") %>% max() + 1},
  max_index = nrow(data),
  shuffle = TRUE,
  step = step,
  batch_size = batch_size
)

# This is how many steps to draw from `val_gen`
# in order to see the whole validation set:
val_steps <- ({which(data$pid=="MJ8002") %>% max()} - {which(data$pid=="MC7070") %>% max() + 1} - lookback) / batch_size
val_steps

test_steps <- (nrow(data) - {which(data$pid=="PC6771") %>% max() + 1} - lookback) / batch_size
test_steps

val_steps <- 6000/batch_size
val_steps

test_steps <- 6000/batch_size
test_steps 
```

### A basic machine learning approach

```{r, echo=TRUE, results='hide'}

set.seed(1)

model <- keras_model_sequential() %>% 
  layer_flatten(input_shape = c(lookback / step, 3)) %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = "accuracy"
)

system.time(history <- model %>% fit(
  train_gen,
  steps_per_epoch = 100,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
))

model %>% evaluate(test_gen, steps=test_steps)
```

```{r}
plot(history)
```

## A first recurrent baseline

```{r}
model_r <- keras_model_sequential() %>% 
  layer_gru(units = 16, input_shape = c(lookback / step, 3)) %>% 
  layer_dense(units = 1)

model_r %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = "accuracy"
)

system.time(history_r <- model_r %>% fit_generator(
  train_gen,
  steps_per_epoch = 100,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
))

model_r %>% evaluate(test_gen, steps=test_steps)
```

```{r}
plot(history_r)
```

## Using recurrent dropout to fight overfitting

```{r}
model_rd <- keras_model_sequential() %>% 
  layer_gru(units = 16, dropout = 0.2, recurrent_dropout = 0.2,
            input_shape = c(lookback / step, 3)) %>% 
  layer_dense(units = 1)

model_rd %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = "accuracy"
)

system.time(history_rd <- model_rd %>% fit_generator(
  train_gen,
  steps_per_epoch = 100,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
))

model_rd %>% evaluate(test_gen, steps=test_steps)
```

```{r}
plot(history_rd)
```

## Stacking recurrent layers

```{r}
model_sr <- keras_model_sequential() %>% 
  layer_gru(units = 16, 
            dropout = 0.1, 
            recurrent_dropout = 0.5,
            return_sequences = TRUE,
            input_shape = c(lookback / step, 3)) %>% 
  layer_gru(units = 16, activation = "relu",
            dropout = 0.1,
            recurrent_dropout = 0.5) %>% 
  layer_dense(units = 1)

model_sr %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = "accuracy"
)

system.time(history_sr <- model_sr %>% fit_generator(
  train_gen,
  steps_per_epoch = 100,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
))

model_sr %>% evaluate(test_gen, steps=test_steps)
```

```{r}
plot(history_sr)
```

## Using bidirectional RNNs

```{r}
model_bR <- keras_model_sequential() %>% 
  bidirectional(
    layer_gru(units = 16), input_shape = c(lookback / step, 3)
  ) %>% 
  layer_dense(units = 1)

model_bR %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = "accuracy"
)

system.time(history_bR <- model_bR %>% fit_generator(
  train_gen,
  steps_per_epoch = 100,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
))

model_bR %>% evaluate(test_gen, steps=test_steps)
```

```{r}
plot(history_bR)
```

>





## Combining CNNs and RNNs to process long sequences

```{r}
model_C <- keras_model_sequential() %>% 
  layer_conv_1d(filters = 16, kernel_size = 5, activation = "relu",
                input_shape = c(lookback / step, 3)) %>% 
  layer_max_pooling_1d(pool_size = 3) %>% 
  layer_conv_1d(filters = 16, kernel_size = 5, activation = "relu") %>% 
  layer_max_pooling_1d(pool_size = 3) %>% 
  layer_conv_1d(filters = 16, kernel_size = 5, activation = "relu") %>% 
  layer_global_max_pooling_1d() %>% 
  layer_dense(units = 1)


model_C %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = "acc"
)

system.time(history_C <- model_C %>% fit_generator(
  train_gen,
  steps_per_epoch = 100,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
))

model_C %>% evaluate(test_gen, steps=test_steps)
```

```{r}
plot(history_C)
```

This is the model, starting with two `layer_conv_1d()` and following up with a `layer_gru()`:

```{r}
model_CR <- keras_model_sequential() %>% 
  layer_conv_1d(filters = 16, kernel_size = 5, activation = "relu",
                input_shape = c(lookback / step, 3)) %>% 
  layer_max_pooling_1d(pool_size = 3) %>% 
  layer_conv_1d(filters = 16, kernel_size = 5, activation = "relu") %>% 
  layer_gru(units = 16, dropout = 0.1, recurrent_dropout = 0.5) %>% 
  layer_dense(units = 1)

summary(model_CR)
```

```{r}
model_CR %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = "acc"
)

system.time(history_CR <- model_CR %>% fit_generator(
  train_gen,
  steps_per_epoch = 100,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
))

model_CR %>% evaluate(test_gen, steps=test_steps)
```

```{r}
plot(history_CR)
```

> 